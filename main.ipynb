{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines the methods used to generate the models discussed in the CARD:Shark 3 paper."
   ]
  },
  {
   "source": [
    "# Parameter(s)\n",
    "\n",
    "- `SUBSAMPLING` (bool): adjust if you would like to subsample the training/testing data to make them balanced"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this if you would like to subsample the data\n",
    "SUBSAMPLING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import (linear_model,\n",
    "                     naive_bayes,\n",
    "                     ensemble,\n",
    "                     svm,\n",
    "                     model_selection,\n",
    "                     metrics,\n",
    "                     model_selection)\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from numpy import interp\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns"
   ]
  },
  {
   "source": [
    "# Paper loading and processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load positive and negative papers\n",
    "def process_text(df):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    processed_text = []\n",
    "    for column in ['text']:\n",
    "        for i, sentence in enumerate(df[column]):\n",
    "            # Convert to lower-case\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "            # Removing punctuation\n",
    "            sentence = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n",
    "            sentence = re.sub(r'[.|,|)|(|\\|/]', r' ', sentence)\n",
    "\n",
    "            # Removing stop words and stemming\n",
    "            stemmer = PorterStemmer()\n",
    "            words = [stemmer.stem(word) for word in sentence.split(\n",
    "            ) if word not in stopwords.words('english')]\n",
    "\n",
    "            # Removing digits\n",
    "            words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "            processed_text.append(' '.join(words))\n",
    "\n",
    "        df['processed_%s' % column] = processed_text\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "card_dataset_df_file = \"card_papers.pkl\"\n",
    "\n",
    "with open(card_dataset_df_file, 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "\n",
    "if \"processed_text\" not in df:\n",
    "    df = process_text(df)\n",
    "    with open(card_dataset_df_file, 'wb') as file:\n",
    "        pickle.dump(df, file)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Subsampling text\n",
    "To test subsampling, change `SUBSAMPLING` to `True`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to test subsampling\n",
    "if SUBSAMPLING:\n",
    "    BASE_PATH = os.path.join('results', 'subsampled')\n",
    "    df = df.loc[df['label'] == 0].sample(1000, random_state=100).append(df.loc[df['label'] == 1].sample(1000, random_state=100))\n",
    "else:\n",
    "    BASE_PATH = os.path.join('results', 'non-subsampled')\n",
    "\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    os.makedirs(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "source": [
    "# Class used to organize models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that contains the function used to organize ml models\n",
    "class ML:\n",
    "    def __init__(self, model, model_name, feature, feature_name, processed_text, parameters={}):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.feature = feature\n",
    "        self.feature_name = feature_name\n",
    "        self.parameters = parameters\n",
    "        self.pipeline = None\n",
    "        self.stats = {}\n",
    "        self.processed_text = processed_text\n",
    "\n",
    "    def create_pipeline(self):\n",
    "        pipeline = Pipeline(\n",
    "            self.feature +\n",
    "            [(self.model_name, self.model)]\n",
    "        )\n",
    "\n",
    "        param_keys = pipeline.get_params()\n",
    "        params = dict(filter(lambda x: x[0] in param_keys,\n",
    "                             self.parameters.items()))\n",
    "        pipeline.set_params(**params)\n",
    "        self.pipeline = pipeline\n",
    "\n",
    "    def set_pipeline(self, X, y):\n",
    "        return self.pipeline.fit(X, y)\n",
    "\n",
    "    def get_stats(self):\n",
    "        return self.stats\n",
    "        \n",
    "    def get_pipeline(self):\n",
    "        return self.pipeline\n",
    "\n",
    "def cross_validate(model, X_cv, y_cv, seed=100, splits=5):\n",
    "    cv = model_selection.StratifiedKFold(\n",
    "        n_splits=splits, random_state=seed, shuffle=True)\n",
    "\n",
    "    cv_results = {}\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(X_cv, y_cv)):\n",
    "\n",
    "        pipeline = model.pipeline.fit(X_cv[train_index], y_cv[train_index])\n",
    "\n",
    "        probas_ = pipeline.predict_proba(X_cv[test_index])\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(\n",
    "            y_cv[test_index], probas_[:, 1])\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        precision, recall, _ = metrics.precision_recall_curve(\n",
    "            y_cv[test_index], probas_[:, 1])\n",
    "\n",
    "        predictions = pipeline.predict(X_cv[test_index])\n",
    "        cm = metrics.confusion_matrix(y_cv[test_index], predictions)\n",
    "        mse = metrics.mean_squared_error(y_cv[test_index], predictions)\n",
    "        cv_results[i] = dict(\n",
    "            fpr=fpr,\n",
    "            tpr=tpr,\n",
    "            thresholds=thresholds,\n",
    "            precision=precision,\n",
    "            recall=recall,\n",
    "            cm=cm,\n",
    "            auc=auc,\n",
    "            mse=mse\n",
    "        )\n",
    "\n",
    "    # Get final stats\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    aucs = []\n",
    "    tprs = []\n",
    "    for _, result in cv_results.items():\n",
    "        auc = metrics.auc(result['fpr'], result['tpr'])\n",
    "        aucs.append(auc)\n",
    "        tpr = interp(mean_fpr, result['fpr'], result['tpr'])\n",
    "        tpr[0] = 0.0\n",
    "        tprs.append(tpr)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    std = np.std(tprs, axis=0)\n",
    "    mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    tprs_upper = np.minimum(mean_tpr+std, 1)\n",
    "    tprs_lower = mean_tpr - std\n",
    "\n",
    "\n",
    "    cv_results['total'] = dict(\n",
    "        mean_auc=mean_auc,\n",
    "        std=std,\n",
    "        tprs_upper=tprs_upper,\n",
    "        tprs_lower=tprs_lower,\n",
    "        std_auc=std_auc,\n",
    "        mean_fpr=mean_fpr,\n",
    "        mean_tpr=mean_tpr\n",
    "    )\n",
    "    model.stats = cv_results\n",
    "    return model"
   ]
  },
  {
   "source": [
    "## Creating combinations between model and feature extraction methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "MODELS = dict(\n",
    "    lr=linear_model.LogisticRegression(random_state=100),\n",
    "    nb=naive_bayes.MultinomialNB(),\n",
    "    rf=ensemble.RandomForestClassifier(random_state=100),\n",
    "    xgb=xgboost.XGBClassifier(),\n",
    "    svm=svm.SVC(probability=True)\n",
    ").items()\n",
    "\n",
    "FEATURES = dict(\n",
    "    count_word=[('count-word', CountVectorizer())],\n",
    "    tf_word=[\n",
    "        ('count-word', CountVectorizer()),\n",
    "        ('tf-idf', TfidfTransformer())],\n",
    "    tf_bitrigram=[\n",
    "        ('count-word', CountVectorizer(ngram_range=(2,3))),\n",
    "        ('tf-idf', TfidfTransformer())]\n",
    "    ).items()\n",
    "\n",
    "combinations = list(itertools.product(MODELS, FEATURES))\n",
    "\n",
    "print('There are %s different combinations to examine' % len(combinations))"
   ]
  },
  {
   "source": [
    "# Functions for cross-validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_splits(df, processed_text):\n",
    "    if processed_text:\n",
    "        papers = df['processed_text'].values\n",
    "    else:\n",
    "        papers = df['text'].values\n",
    "    labels = df['label'].values.astype(\"int32\")\n",
    "    X_cv, X_holdout, y_cv, y_holdout = model_selection.train_test_split(\n",
    "                papers, labels, random_state=100)\n",
    "    return [X_cv, X_holdout, y_cv, y_holdout]\n",
    "\n",
    "def run_pipelines(splits, processed_text):\n",
    "    ml_pipelines = []\n",
    "    X_cv, X_holdout, y_cv, y_holdout = splits\n",
    "    for ((model_name, model_fx), (feature_name, feature_fx)) in tqdm(combinations, desc=\"Models (Processed text: %s)\" % (processed_text)):\n",
    "        ml = ML(model_fx, model_name, feature_fx, feature_name, processed_text)\n",
    "        ml.create_pipeline()\n",
    "        ml = cross_validate(ml, X_cv, y_cv)\n",
    "        ml_pipelines.append(ml)\n",
    "    return ml_pipelines\n",
    "\n",
    "def create_models():\n",
    "    unprocessed_splits = get_splits(df, False)\n",
    "    processed_splits = get_splits(df, True)\n",
    "    return run_pipelines(unprocessed_splits, False) + run_pipelines(processed_splits, True)"
   ]
  },
  {
   "source": [
    "# CARD:Shark 2 methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import card_shark_functions \n",
    "from card_shark_functions import *\n",
    "\n",
    "def run_card_shark():\n",
    "    seed = 100 \n",
    "    exclude = [\"in\", \"and\", \"the\", \"or\", \"if\", \"a\", \"of\", \"from\", \"by\", \"with\"]\n",
    "    text_processed = [False, True]\n",
    "\n",
    "    cv = model_selection.StratifiedKFold(\n",
    "            n_splits=5, random_state=100, shuffle=True)\n",
    "\n",
    "    for processed in text_processed:\n",
    "        ml = ML(None, \"card_shark\", None, None, processed)\n",
    "        cv_results = {}\n",
    "        (cardWords, cardJournals, cardAbstracts) = wordFrequency(df.loc[df['label']==1], exclude, process=processed) \n",
    "\n",
    "        X_cv, _, y_cv, _ = get_splits(df, processed)\n",
    "        for i, (train_index, test_index) in enumerate(tqdm(list(cv.split(X_cv, y_cv)), desc=\"CARD:Shark (Processed text: %s)\" % (processed))):\n",
    "            df_ = []\n",
    "            for z in X_cv:\n",
    "                if processed:\n",
    "                    df_.append(df.loc[df['processed_text'] == z].copy())\n",
    "                else:\n",
    "                    df_.append(df.loc[df['text'] == z].copy())\n",
    "            df_ = pd.concat(df_, axis=0)\n",
    "            df_ = df_.reset_index()\n",
    "            df_train = df_.iloc[train_index]\n",
    "            df_test = df_.iloc[test_index]\n",
    "\n",
    "            (sampleWords, sampleJournals, sampleAbstracts) = wordFrequency(df_train, exclude, process=processed)\n",
    "            matrixA = matrixMaker(cardWords, sampleWords, True)\n",
    "            matrixD = doubleMatrixMaker(cardAbstracts, sampleAbstracts, exclude)\n",
    "            matrixJ = matrixMaker(cardJournals, sampleJournals, True)\n",
    "            stats, _ = bluePill('text_pmids', matrixA, matrixJ, matrixD, exclude, df_test, processed)\n",
    "            cv_results[i] = stats\n",
    "\n",
    "\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        aucs = []\n",
    "        tprs = []\n",
    "        for _, result in cv_results.items():\n",
    "            auc = metrics.auc(result['fpr'], result['tpr'])\n",
    "            aucs.append(auc)\n",
    "            tpr = interp(mean_fpr, result['fpr'], result['tpr'])\n",
    "            tpr[0] = 0.0\n",
    "            tprs.append(tpr)\n",
    "\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        std = np.std(tprs, axis=0)\n",
    "        mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "\n",
    "        tprs_upper = np.minimum(mean_tpr+std, 1)\n",
    "        tprs_lower = mean_tpr - std\n",
    "\n",
    "        cv_results['total'] = dict(\n",
    "            mean_auc=mean_auc,\n",
    "            std=std,\n",
    "            tprs_upper=tprs_upper,\n",
    "            tprs_lower=tprs_lower,\n",
    "            std_auc=std_auc,\n",
    "            mean_fpr=mean_fpr,\n",
    "            mean_tpr=mean_tpr\n",
    "        )\n",
    "        ml.stats = cv_results\n",
    "        yield ml"
   ]
  },
  {
   "source": [
    "# Running cross-validation on CARD:Shark 2 and 3\n",
    "\n",
    "Loads pipelines if they exist, otherwise creates and saves them"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_PATH = os.path.join(BASE_PATH, 'all_pipelines.pkl')\n",
    "try:\n",
    "    with open(PIPELINE_PATH, 'rb') as file:\n",
    "        all_pipelines = pickle.load(file)\n",
    "except:\n",
    "    all_pipelines = create_models() + [x for x in run_card_shark()]\n",
    "    with open(PIPELINE_PATH, 'wb') as file:\n",
    "        pickle.dump(all_pipelines, file)"
   ]
  },
  {
   "source": [
    "# Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "colors = dict(\n",
    "    nb=c[0],\n",
    "    lr=c[1],\n",
    "    rf=c[2],\n",
    "    xgb=c[3],\n",
    "    svm=c[4],\n",
    "    card_shark=c[5]\n",
    ")\n",
    "names = dict(\n",
    "    nb=\"Naive Bayes\",\n",
    "    lr=\"Logistic regression\",\n",
    "    rf=\"Random forest\",\n",
    "    xgb=\"Extreme gradient boosting\",\n",
    "    svm=\"Support vector machine\",\n",
    "    card_shark=\"CARD:Shark 2\"\n",
    ")\n",
    "features = dict(\n",
    "    count_word=\"Word count\",\n",
    "    tf_word=\"TF-IDF Word\",\n",
    "    tf_bitrigram=\"TF-IDF Bi/tri-gram\"\n",
    ")\n",
    "lines = dict(\n",
    "    count_word=\"solid\",\n",
    "    tf_word=\"dotted\",\n",
    "    tf_bitrigram=\"dashed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_roc(ml_items, process, save=False):\n",
    "    plt.clf()\n",
    "    fig = plt.figure(dpi=300)\n",
    "    fig.set_figwidth(9)\n",
    "    fig.set_figheight(8)\n",
    "    ax = plt.axes(xlim=(-0.05,1.05), ylim=(-0.05,1.05))\n",
    "    plt.xticks([0.0,0.2,0.4,0.6,0.8,1.0])\n",
    "    plt.yticks([0.0,0.2,0.4,0.6,0.8,1.0])\n",
    "\n",
    "    ax.plot([0,1], [0,1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    for ml in ml_items:\n",
    "        if ml.processed_text != process:\n",
    "            continue\n",
    "        model_name = names.get(ml.model_name)\n",
    "        feature_name = features.get(ml.feature_name)\n",
    "        color = colors.get(ml.model_name)\n",
    "        line = lines.get(ml.feature_name)\n",
    "        stats = ml.stats['total']\n",
    "        mean_fpr = stats['mean_fpr']\n",
    "        mean_tpr = stats['mean_tpr']\n",
    "        mean_auc = stats['mean_auc']\n",
    "        ax.plot(mean_fpr, mean_tpr, alpha=0.9, label=r'Mean ROC %s, %s (AUC = %0.2f)' % (model_name, feature_name, mean_auc), color=color, linestyle=line)\n",
    "        ax.fill_between(mean_fpr, stats['tprs_lower'], stats['tprs_upper'], alpha = 0.2, color=color)\n",
    "\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(save+'.png')\n",
    "        plt.savefig(save+'.svg', format=\"svg\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print(\"Unprocessed text ROC\")\n",
    "create_roc(all_pipelines, False, os.path.join(BASE_PATH, 'unprocessed_text_roc'))\n",
    "print(\"Processed text ROC\")\n",
    "create_roc(all_pipelines, True, os.path.join(BASE_PATH, 'processed_text_roc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_table(all_pipelines, save=False):\n",
    "    results = []\n",
    "    for ml in all_pipelines:\n",
    "        cm = sum([y['cm'].ravel() for x,y in ml.stats.items() if x != 'total'])\n",
    "        precision = cm[3] / (cm[3]+cm[1])\n",
    "        recall = cm[3] / (cm[3]+cm[2])\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "        results.append(dict(model_name=names[ml.model_name], feature_name=features.get(ml.feature_name), processed_text=ml.processed_text, precision=precision, recall=recall, f1=f1))\n",
    "    table_df = pd.DataFrame(results)\n",
    "    table_df = table_df.reset_index(drop=True).set_index(['model_name', 'feature_name', 'processed_text'])\n",
    "\n",
    "    if save:\n",
    "        table_df.to_csv(save)\n",
    "    return table_df\n",
    "\n",
    "table = create_table(all_pipelines, os.path.join(BASE_PATH, 'testing_results_table.csv'))   \n",
    "table"
   ]
  },
  {
   "source": [
    "# Creating predictions on validation set (papers from Sep and Nov 2019)\n",
    "\n",
    "This step can take several hours to run (took me aprox. 10 hours)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PAPER_PATH = os.path.join('paper_download', 'out')\n",
    "nov_papers = pd.read_json(os.path.join(PAPER_PATH, 'pubmed_papers-nov_1-30.json'))\n",
    "sep_papers = pd.read_json(os.path.join(PAPER_PATH, 'pubmed_papers-sep_1-30.json'))\n",
    "\n",
    "def get_text(paper_df, processed):\n",
    "    if processed:\n",
    "        return paper_df['processed_text'].values\n",
    "    else:\n",
    "        return paper_df['text'].values\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "def fit_final_and_save(ml, filename):\n",
    "    pipeline = ml.get_pipeline()\n",
    "    X_cv, _, y_cv, _ = get_splits(df, ml.processed_text)\n",
    "    pipeline = pipeline.fit(X_cv, y_cv) \n",
    "    directory = os.path.join(BASE_PATH, 'final_models')\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    path = os.path.join(directory, filename+'.pkl')\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(pipeline, file)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "for ml in tqdm(all_pipelines[::-1]):\n",
    "    nov_text = get_text(nov_papers, ml.processed_text)\n",
    "    sep_text = get_text(sep_papers, ml.processed_text)\n",
    "\n",
    "    filename = '%s-%s-%s' % (ml.model_name, ml.feature_name, ml.processed_text)\n",
    "    if ml.model_name == 'card_shark':\n",
    "        X_cv, _, y_cv, _ = get_splits(df, ml.processed_text)\n",
    "        df_ = []\n",
    "        for z in X_cv:\n",
    "            if ml.processed_text:\n",
    "                df_.append(df.loc[df['processed_text'] == z].copy())\n",
    "            else:\n",
    "                df_.append(df.loc[df['text'] == z].copy())\n",
    "        df_ = pd.concat(df_, axis=0)\n",
    "        df_ = df_.reset_index()\n",
    "        exclude = [\"in\", \"and\", \"the\", \"or\", \"if\", \"a\", \"of\", \"from\", \"by\", \"with\"]\n",
    "        processed = ml.processed_text\n",
    "        (cardWords, cardJournals, cardAbstracts) = wordFrequency(df.loc[df['label']==1], exclude, process=processed) \n",
    "        (sampleWords, sampleJournals, sampleAbstracts) = wordFrequency(df_, exclude, process=processed)\n",
    "        matrixA = matrixMaker(cardWords, sampleWords, True)\n",
    "        matrixD = doubleMatrixMaker(cardAbstracts, sampleAbstracts, exclude)\n",
    "        matrixJ = matrixMaker(cardJournals, sampleJournals, True)\n",
    "        prediction_1 = bluePill('text_pmids', matrixA, matrixJ, matrixD, exclude, nov_papers, processed, True)\n",
    "        prediction_2 = bluePill('text_pmids', matrixA, matrixJ, matrixD, exclude, sep_papers, processed, True)\n",
    "        predictions[filename] = list(zip(nov_papers['pmid'], prediction_1)) + list(zip(sep_papers['pmid'], prediction_2))\n",
    "    else:\n",
    "        pipeline = fit_final_and_save(ml, filename)\n",
    "        a = list(zip(nov_papers['pmid'], pipeline.predict(nov_text)))\n",
    "        b = list(zip(sep_papers['pmid'], pipeline.predict(sep_text)))\n",
    "        predictions[filename] = a + b\n",
    "\n",
    "def convert(o):\n",
    "    if isinstance(o, np.int32): return int(o)  \n",
    "    raise TypeError\n",
    "\n",
    "with open(os.path.join(BASE_PATH, 'all_predictions.json'), 'w') as file:\n",
    "    json.dump(predictions, file, default=convert)    \n"
   ]
  },
  {
   "source": [
    "## Parsing raw validation data\n",
    "\n",
    "See `./validation_distribution` for methods used there."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from validation_distribution import validation_visualization as vv\n",
    "from importlib import reload  \n",
    "reload(vv)\n",
    "\n",
    "config = os.path.join('validation_distribution', 'resources', 'config.json')\n",
    "prediction_path = os.path.join(BASE_PATH, 'all_predictions.json')\n",
    "validation_out, b = vv.main(config, [prediction_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_total = {}\n",
    "for _, item in validation_out.items():\n",
    "    for method, results in item['acc'].items():\n",
    "        for result_name, n in results.items():\n",
    "            if method in final_total:\n",
    "                final_total[method][result_name] += n\n",
    "            else:\n",
    "                final_total[method] = results\n",
    "                break"
   ]
  },
  {
   "source": [
    "## Stats on validation set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_df = pd.DataFrame(final_total).transpose()/2 # Need to remove the duplicates from validation\n",
    "validation_df['Model'] = [names.get(x.split('-')[0][6:]) for x in validation_df.index]\n",
    "validation_df['Feature'] = [features.get(x.split('-')[1]) for x in validation_df.index]\n",
    "validation_df['Processed text'] = [x.split('-')[-1] for x in validation_df.index]\n",
    "validation_df = validation_df.reset_index(drop=True).set_index(['Model', 'Feature', 'Processed text'])\n",
    "validation_df['Precision'] = validation_df['TP'] / (validation_df['TP'] + validation_df['FP'])\n",
    "validation_df['Recall'] = validation_df['TP'] / (validation_df['TP'] + validation_df['FN'])\n",
    "validation_df['f1'] = 2*validation_df['TP'] / (2*validation_df['TP'] + validation_df['FP'] + validation_df['FN'])\n",
    "validation_df.to_csv(os.path.join(BASE_PATH, 'validation_results.csv'))\n",
    "validation_df"
   ]
  },
  {
   "source": [
    "## Stats on total +ve/-ve predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions():\n",
    "    with open(os.path.join(BASE_PATH, 'all_predictions.json'), 'r') as file:\n",
    "        all_predictions = json.load(file)\n",
    "\n",
    "    out = []\n",
    "    for name, results in all_predictions.items():\n",
    "        model = names.get(name.split('-')[0])\n",
    "        feature = features.get(name.split('-')[1])\n",
    "        process = name.split('-')[-1] \n",
    "        positives = sum([1 for x in results if x[1] == 1])\n",
    "        negatives = sum([1 for x in results if x[1] == 0])\n",
    "        out.append(dict(model=model, feature=feature, process=process,positives=positives,negatives=negatives))\n",
    "    return pd.DataFrame(out).reset_index(drop=True).set_index(['model', 'feature', 'process'])\n",
    "\n",
    "validation_predictions = get_predictions()\n",
    "validation_predictions.to_csv(os.path.join(BASE_PATH, 'model_sep_nov_predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_predictions"
   ]
  },
  {
   "source": [
    "# Gridsearch hyperparameter tuning\n",
    "\n",
    "Using the three chosen pipelines:\n",
    "\n",
    "- logistic regression/TF-IDF word/processed\n",
    "- naive bayes/word count/processed\n",
    "- random forest/TF-IDF bi/tri-gram/processed\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.logspace(-4, 4, 10)\n",
    "PARAMETERS = dict(\n",
    "    lr=[dict(\n",
    "            lr__penalty=['l1'],\n",
    "            lr__solver=['liblinear', 'saga'],\n",
    "            lr__C=C,\n",
    "            lr__max_iter=[10,50,100,150],\n",
    "            lr__random_state=[100]\n",
    "            ),\n",
    "        dict(\n",
    "            lr__penalty=['l2'],\n",
    "            lr__solver=['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "            lr__C=C,\n",
    "            lr__max_iter=[10,50,100,150],\n",
    "            lr__random_state=[100]\n",
    "            )\n",
    "            ],\n",
    "\n",
    "    rf=dict(\n",
    "        rf__n_estimators = [int(x) for x in np.linspace(100, 1000, 5)][::-1],\n",
    "        rf__max_features=['sqrt', 'log2'],\n",
    "        rf__min_samples_leaf=[1, 4, 10, 30, 60],\n",
    "        rf__random_state=[100]\n",
    "        ),\n",
    "\n",
    "    count_word={\n",
    "        \"count-word__max_df\":[0.5,0.75,1],\n",
    "        \"count-word__max_features\":[None, 5000, 10000, 50000]\n",
    "    })\n",
    "\n",
    "GRID_PATH = os.path.join('results', 'non-subsampled', 'final_models')\n",
    "file_names = ['lr-tf_word-True.pkl', 'nb-count_word-True.pkl', 'rf-tf_bitrigram-True.pkl']\n",
    "processed_splits = get_splits(df, True)\n",
    "for file in file_names:\n",
    "    PATH = os.path.join(GRID_PATH, file)\n",
    "    with open(PATH, 'rb') as f:\n",
    "        pl = pickle.load(f)\n",
    "    steps = [step[0].replace('-', '_') for step in pl.steps]\n",
    "    pl_parameters = {}\n",
    "    for step_name, parameters in PARAMETERS.items():\n",
    "        if step_name not in steps:\n",
    "            continue\n",
    "        if step_name == 'lr':\n",
    "            pl_parameters = parameters\n",
    "            [param.update(PARAMETERS['count_word']) for param in pl_parameters]\n",
    "            break\n",
    "        pl_parameters.update(parameters)\n",
    "    grid_search = GridSearchCV(pl, pl_parameters, verbose=1, n_jobs=4, refit=True)\n",
    "    grid_search.fit(processed_splits[0], processed_splits[2])\n",
    "    estimator = grid_search.best_estimator_\n",
    "    with open(os.path.join(GRID_PATH, 'grid_search_out-'+file), 'wb') as f:\n",
    "        pickle.dump(estimator, f)"
   ]
  },
  {
   "source": [
    "## Testing final models on hold-out data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['grid_search_out-lr-tf_word-True.pkl', 'grid_search_out-nb-count_word-True.pkl', 'grid_search_out-rf-tf_bitrigram-True.pkl']\n",
    "\n",
    "def holdout_eval(pipeline, X_holdout, y_holdout):\n",
    "    probas_ = pipeline.predict_proba(X_holdout)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_holdout, probas_[:, 1])\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    precision, recall, _ = metrics.precision_recall_curve(\n",
    "        y_holdout, probas_[:, 1])\n",
    "\n",
    "    predictions = pipeline.predict(X_holdout)\n",
    "    p, r, f1, _ = metrics.precision_recall_fscore_support(y_holdout, predictions, average=\"weighted\")\n",
    "    cm = metrics.confusion_matrix(y_holdout, predictions)\n",
    "    mse = metrics.mean_squared_error(y_holdout, predictions)\n",
    "\n",
    "    results = dict(\n",
    "        fpr=fpr,\n",
    "        tpr=tpr,\n",
    "        thresholds=thresholds,\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        cm=cm,\n",
    "        auc=auc,\n",
    "        mse=mse,\n",
    "        p=p,\n",
    "        r=r,\n",
    "        f1=f1,\n",
    "        mean_fpr=np.linspace(0, 1, 100)\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "processed_splits = get_splits(df, True)\n",
    "holdout_results = []\n",
    "for file_name in file_names:\n",
    "    with open(os.path.join(GRID_PATH, file_name), 'rb') as f:\n",
    "        pl = pickle.load(f)\n",
    "    \n",
    "    holdout_result = holdout_eval(pl, processed_splits[1], processed_splits[3])\n",
    "    holdout_results.append((file_name, holdout_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roc(ml_items, save=False):\n",
    "    plt.clf()\n",
    "    fig = plt.figure(dpi=300)\n",
    "    fig.set_figwidth(9)\n",
    "    fig.set_figheight(8)\n",
    "    ax = plt.axes(xlim=(-0.05,1.05), ylim=(-0.05,1.05))\n",
    "    plt.xticks([0.0,0.2,0.4,0.6,0.8,1.0])\n",
    "    plt.yticks([0.0,0.2,0.4,0.6,0.8,1.0])\n",
    "\n",
    "    ax.plot([0,1], [0,1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    for (name, result) in ml_items:\n",
    "        model_name = names.get(name.split('-')[1])\n",
    "        feature_name = features.get(name.split('-')[2])\n",
    "        color = colors.get(model_name)\n",
    "        fpr = result['fpr']\n",
    "        tpr = result['tpr']\n",
    "        auc = result['auc']\n",
    "        ax.plot(fpr, tpr, alpha=0.9, label=r'ROC %s, %s (AUC = %0.2f)' % (model_name, feature_name, auc), color=color)\n",
    "\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(save+'.png')\n",
    "        plt.savefig(save+'.svg', format=\"svg\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "create_roc(holdout_results, os.path.join('results', 'non-subsampled', 'grid_search_roc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(ml_items, save=False):\n",
    "    results = []\n",
    "    for (name, result) in ml_items:\n",
    "        model_name = names.get(name.split('-')[1])\n",
    "        feature_name = features.get(name.split('-')[2])\n",
    "\n",
    "        precision = result['p']\n",
    "        recall = result['r']\n",
    "        f1 = result['f1']\n",
    "        results.append(dict(model_name=model_name, feature_name=feature_name, precision=precision, recall=recall, f1=f1))\n",
    "    table_df = pd.DataFrame(results)\n",
    "    table_df = table_df.reset_index(drop=True).set_index(['model_name', 'feature_name'])\n",
    "\n",
    "    if save:\n",
    "        table_df.to_csv(save)\n",
    "    return table_df\n",
    "\n",
    "table = create_table(holdout_results, os.path.join('results', 'non-subsampled', 'grid_search_holdout_results.csv'))   \n",
    "table"
   ]
  },
  {
   "source": [
    "## Retrospective comparison between CS2 and CS3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_PATH = os.path.join('results', 'non-subsampled', 'retrospective_predictions')\n",
    "PAPER_PATH = os.path.join('paper_download', 'out', 'jun-2017_to_dec-2020')\n",
    "file_names = ['grid_search_out-lr-tf_word-True.pkl', 'grid_search_out-nb-count_word-True.pkl', 'grid_search_out-rf-tf_bitrigram-True.pkl']\n",
    "PATH = os.path.join('results', 'non-subsampled', 'final_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PREDICTIONS_PATH):\n",
    "    os.makedirs(PREDICTIONS_PATH)\n",
    "\n",
    "for file_name in file_names:\n",
    "    pl_predictions = set()\n",
    "    with open(os.path.join(PATH, file_name), 'rb') as f:\n",
    "        pl = pickle.load(f)\n",
    "    \n",
    "    for paper_file in os.listdir(PAPER_PATH):\n",
    "        if not paper_file.endswith('.json'):\n",
    "            continue\n",
    "        papers = pd.read_json(os.path.join(PAPER_PATH, paper_file))\n",
    "\n",
    "        predictions = pl.predict(papers['processed_text'])\n",
    "        pl_predictions |= {pmid for pmid, pred in zip(papers['pmid'], predictions) if pred == 1}\n",
    "\n",
    "    with open(os.path.join(PREDICTIONS_PATH, '%s-predictions.json' % file_name), 'w') as f:\n",
    "        json.dump(list(pl_predictions), f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_PAPER_PMID_PATH = os.path.join('paper_download', 'out', 'jun-2017_to_dec-2020_pmids.json')\n",
    "\n",
    "all_papers = set()\n",
    "\n",
    "if os.path.exists(TOTAL_PAPER_PMID_PATH):\n",
    "    with open(TOTAL_PAPER_PMID_PATH, 'r') as f:\n",
    "        all_papers = set(json.load(f))\n",
    "\n",
    "else:\n",
    "    for paper_file in os.listdir(PAPER_PATH):\n",
    "        if not paper_file.endswith('.json'):\n",
    "            continue\n",
    "        papers = pd.read_json(os.path.join(PAPER_PATH, paper_file))\n",
    "        all_papers |= set(papers['pmid'])\n",
    "    with open(TOTAL_PAPER_PMID_PATH, 'w') as f:\n",
    "        json.dump(list(all_papers), f)\n",
    "\n",
    "all_papers = {str(x) for x in all_papers}\n",
    "print(len(all_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_3_predictions = {}\n",
    "for file_name in os.listdir(PREDICTIONS_PATH):\n",
    "    with open(os.path.join(PREDICTIONS_PATH, file_name), 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    name = file_name.split('-')[1]\n",
    "    cs_3_predictions[name] = set(predictions)\n",
    "\n",
    "for name, item in cs_3_predictions.items():\n",
    "    cs_3_predictions[name] = {str(x) for x in item}\n",
    "\n",
    "with open(os.path.join('retrospective_cs2_pred', 'out', 'cs_2_predictions.json'), 'r') as f:\n",
    "    cs_2_predictions = json.load(f)\n",
    "\n",
    "card_papers_path = 'card_pubs.csv'\n",
    "card_papers = pd.read_csv(card_papers_path)\n",
    "original_card_pmids = set(card_papers['accession'])\n",
    "# Filter down to pmids found in the 3 million papers examined by the models\n",
    "card_pmids = original_card_pmids.intersection(all_papers)\n",
    "print(len(card_pmids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retrospective_table(cs_3, cs_2, card, all_papers):\n",
    "    cs_2_high = set(cs_2['high'])\n",
    "    cs_2_low = set(cs_2['low'])\n",
    "    PATH = os.path.join('results', 'non-subsampled')\n",
    "    cs_2_only = (cs_2_high | cs_2_low) - all_papers\n",
    "\n",
    "    cs_2_performance = len(cs_2_high.intersection(card) - cs_2_only)\n",
    "    cs_2_ignored = len(cs_2_low.intersection(card) - cs_2_only - cs_2_high)\n",
    "\n",
    "    rf = ('Random forest', len(all_papers), len(cs_3['rf']-cs_2_only), len(cs_3['rf'].intersection(card)-cs_2_only))\n",
    "    nb = ('Naive Bayes', len(all_papers), len(cs_3['nb']-cs_2_only), len(cs_3['nb'].intersection(card)-cs_2_only))\n",
    "    lr = ('Logistic regression', len(all_papers), len(cs_3['lr']-cs_2_only), len(cs_3['lr'].intersection(card)-cs_2_only))\n",
    "    cs_2 = ('CARD:Shark 2', len((cs_2_high | cs_2_low) - cs_2_only), f'H: {len(cs_2_high - cs_2_only)} L: {len(cs_2_low - cs_2_only - cs_2_high)}', f'H: {cs_2_performance} L: {cs_2_ignored}')\n",
    "\n",
    "    table_df = pd.DataFrame([lr, nb, rf, cs_2], columns=['Model name', 'Papers examined', 'Positive predictions', 'CARD overlap'])\n",
    "    table_df = table_df.reset_index(drop=True).set_index(['Model name'])\n",
    "\n",
    "    table_df.to_csv(os.path.join('results', 'non-subsampled', 'retrospec_results.csv'))\n",
    "\n",
    "create_retrospective_table(cs_3_predictions, cs_2_predictions, card_pmids, all_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from venn import venn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_venn(cs_3, cs_2, card, all_papers):\n",
    "    cs_2_high = set(cs_2['high'])\n",
    "    cs_2_low = set(cs_2['low'])\n",
    "    PATH = os.path.join('results', 'non-subsampled')\n",
    "    cs_2_only = (cs_2_high | cs_2_low) - all_papers\n",
    "    v = {\"Random forest\":cs_3['rf'], \"Naive Bayes\":cs_3['nb'], \"Logistic regression\":cs_3['lr'], \"CARD:Shark 2\":cs_2_high - cs_2_only}\n",
    "    venn(v)\n",
    "    plt.savefig(os.path.join(PATH, 'venn_diagram_overall_overlap.png'))\n",
    "    plt.show()\n",
    "\n",
    "    cs_2_performance = cs_2_high.intersection(card) - cs_2_only\n",
    "    rf_p = cs_3['rf'].intersection(card) - cs_2_only\n",
    "    nb_p = cs_3['nb'].intersection(card) - cs_2_only\n",
    "    lr_p = cs_3['lr'].intersection(card) - cs_2_only\n",
    "\n",
    "    v2 = {\"Random forest\":rf_p, \"Naive Bayes\":nb_p, \"Logistic regression\":lr_p, \"CARD:Shark 2\":cs_2_performance}\n",
    "    venn(v2)\n",
    "    plt.savefig(os.path.join(PATH, 'venn_diagram_card_subset_overlap.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_venn(cs_3_predictions, cs_2_predictions, card_pmids, all_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cs_2_only(cs_2, card):\n",
    "    cs_2_high = set(cs_2['high'])\n",
    "    cs_2_low = set(cs_2['low'])\n",
    "    cs_2_positive = (cs_2_high | cs_2_low).intersection(card)\n",
    "    print(len(cs_2_positive))\n",
    "\n",
    "cs_2_only(cs_2_predictions, card_pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('card_shark_3_paper-441UfaeS')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "interpreter": {
   "hash": "3ff210351e05dc74ac9ab2d59cd8e6bec59a0b2cb73d9707f52a8b8f15b80bce"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}